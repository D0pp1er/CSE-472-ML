{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"Adult/adult.data\"\n",
    "df  = pd.read_csv(train_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = \"Adult/adult.test\"\n",
    "df_test = pd.read_csv(test_file_path, skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "object_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column headers\n",
    "column_headers = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', \n",
    "                  'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
    "                  'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "# Assign column headers to the training DataFrame\n",
    "df.columns = column_headers\n",
    "\n",
    "# Assign column headers to the test DataFrame\n",
    "df_test.columns = column_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate df and df_test\n",
    "df_combined = pd.concat([df, df_test], ignore_index=True)\n",
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = df_combined.select_dtypes(include=['object']).columns\n",
    "for column in object_columns:\n",
    "    df_combined[column] = df_combined[column].astype('category')\n",
    "\n",
    "df_combined.dtypes\n",
    "\n",
    "# df_combined['income'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in object_columns:\n",
    "#     print(f\"Value counts for {column}:\")\n",
    "#     print(df_combined[column].value_counts())\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['income'] = df_combined['income'].str.replace('.', '', regex=False)\n",
    "df_combined['income'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined['income'] = df_combined['income'].map({'<=50K': '<=50K', '>50K': '>50k', '<=50K.': 0, '>50K.': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['income'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "feature_col = \"income\"\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_combined[feature_col] = label_encoder.fit_transform(df_combined[feature_col])\n",
    "\n",
    "for col in df_combined.select_dtypes(include=['category']).columns:\n",
    "    if col != feature_col and df_combined[col].nunique() == 2:\n",
    "\n",
    "        df_combined[col] = label_encoder.fit_transform(df_combined[col])\n",
    "\n",
    "df_combined.dtypes\n",
    "\n",
    "df_combined = pd.get_dummies(df_combined, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print columns grouped by their data types\n",
    "int_columns = df_combined.select_dtypes(include=['int64']).columns.tolist()\n",
    "bool_columns = df_combined.select_dtypes(include=['bool']).columns.tolist()\n",
    "\n",
    "print(\"Integer Columns:\")\n",
    "print(int_columns)\n",
    "print(\"\\nBoolean Columns:\")\n",
    "print(bool_columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all boolean columns to int64\n",
    "df_combined[bool_columns] = df_combined[bool_columns].astype('int64')\n",
    "\n",
    "# Verify the conversion\n",
    "int_columns = df_combined.select_dtypes(include=['int64']).columns.tolist()\n",
    "int_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all int64 columns to float32\n",
    "df_combined[int_columns] = df_combined[int_columns].astype('float32')\n",
    "\n",
    "# Verify the conversion\n",
    "df_combined.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def scale_features(df, scaling_method='standard', target_column=None):\n",
    "    \"\"\"\n",
    "    Scale all features except the target column.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame, the input dataset.\n",
    "    - scaling_method: str, 'standard' for Standard Scaling or 'minmax' for Min-Max Scaling.\n",
    "    - target_column: str, name of the target column (optional).\n",
    "    \n",
    "    Returns:\n",
    "    - df_scaled: DataFrame, DataFrame with scaled features.\n",
    "    \"\"\"\n",
    "    if scaling_method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaling_method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid scaling method. Choose 'standard' or 'minmax'.\")\n",
    "\n",
    "    # Separate features and target variable\n",
    "    if target_column:\n",
    "        features = df.drop(columns=[target_column])\n",
    "        target = df[target_column]\n",
    "    else:\n",
    "        features = df.copy()\n",
    "        target = None\n",
    "\n",
    "    # Scale all features\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    df_scaled = pd.DataFrame(features_scaled, columns=features.columns, index=df.index)\n",
    "\n",
    "    # Add the target column back if it was provided\n",
    "    if target_column:\n",
    "        df_scaled[target_column] = target\n",
    "\n",
    "    return df_scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = scale_features(df_combined, 'standard', 'income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_correlations(df, n):\n",
    "    import numpy as np\n",
    "    correlations = df.corr().abs().unstack().sort_values(ascending=False)\n",
    "    correlations = correlations[correlations < 1]\n",
    "    return correlations[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_correlations_with_target(df, target, n):\n",
    "    correlations = df.corr().abs().unstack().sort_values(ascending=False)\n",
    "    correlations = correlations[correlations < 1]\n",
    "    return correlations[target][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_n_correlations_with_target(df_combined, 'income', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Extract the top 20 features and the target variable\n",
    "top_20_features = get_top_n_correlations_with_target(df_combined, 'income', 20).index.tolist()\n",
    "X = df_combined[top_20_features]\n",
    "y = df_combined['income']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first_dataset_final = df_combined.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seconed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_df_file_path = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "df_third_dataset = pd.read_csv(third_df_file_path)\n",
    "df = df_third_dataset.copy(deep=True)\n",
    "df.head()\n",
    "\n",
    "\n",
    "df.drop('customerID', axis=1, inplace=True)\n",
    "df.dtypes\n",
    "\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df['TotalCharges'].fillna(df['TotalCharges'].mean(), inplace=True)\n",
    "df['TotalCharges'] = df['TotalCharges'].astype('float64')\n",
    "\n",
    "# Verify the conversion and filling\n",
    "df['TotalCharges'].isna().sum()\n",
    "\n",
    "\n",
    "\n",
    "print(f'Null values in each column:\\n{df.isna().sum()}')\n",
    "print(f\"duplicated rows: {df.duplicated().sum()}\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert all object types into category types\n",
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "for column in object_columns:\n",
    "    df[column] = df[column].astype('category')\n",
    "\n",
    "# Verify the conversion\n",
    "df.dtypes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Label encode the target column 'Churn' in the third dataset\n",
    "df['Churn'] = label_encoder.fit_transform(df['Churn'])\n",
    "\n",
    "# Verify the encoding\n",
    "df['Churn'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "for column in df.columns:\n",
    "    print(f\"Unique values and counts for {column}:\")\n",
    "    print(df[column].value_counts())\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "feature_col = 'Churn'\n",
    "for col in df.select_dtypes(include=['category']).columns:\n",
    "    if col != feature_col and df[col].nunique() == 2:\n",
    "\n",
    "        df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "df.dtypes\n",
    "\n",
    "df = pd.get_dummies(df).astype('float64')\n",
    "\n",
    "\n",
    "\n",
    "for column in df.columns:\n",
    "    print(f\"Unique values and counts for {column}:\")\n",
    "    print(df[column].value_counts())\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.duplicated().sum().sum()\n",
    "\n",
    "\n",
    "df.isna().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Churn'] = df['Churn'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Extract the top 20 features and the target variable\n",
    "top_20_features_df = get_top_n_correlations_with_target(df, 'Churn', 20).index.tolist()\n",
    "X = df[top_20_features_df]\n",
    "y = df['Churn']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=100000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_second_dataset_final = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_df_file_path = \"creditcard.csv\"\n",
    "df_third_dataset = pd.read_csv(third_df_file_path)\n",
    "\n",
    "df = df_third_dataset.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scale_features(df, 'standard', 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Extract the top 20 features and the target variable\n",
    "top_20_features_df = get_top_n_correlations_with_target(df, 'Class', 20).index.tolist()\n",
    "X = df[top_20_features_df]\n",
    "y = df['Class']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_third_dataset_final = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first_dataset_final.describe()\n",
    "print(df_first_dataset_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_second_dataset_final.describe()\n",
    "print(df_second_dataset_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_third_dataset_final.describe()\n",
    "print(df_third_dataset_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# class LogisticRegression:\n",
    "#     def __init__(self, learning_rate=0.01, num_iterations=1000, regularization_strength=0.01):\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.num_iterations = num_iterations\n",
    "#         self.regularization_strength = regularization_strength  # L2 regularization strength\n",
    "    \n",
    "#     def sigmoid(self, z):\n",
    "#         # Clip values to avoid overflow\n",
    "#         z = np.clip(z, -500, 500)\n",
    "#         return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "#     def fit(self, X, y):\n",
    "#         self.m, self.n = X.shape\n",
    "#         self.weights = np.zeros(self.n)\n",
    "#         self.bias = 0\n",
    "#         X = np.array(X)\n",
    "#         y = np.array(y)\n",
    "        \n",
    "#         for i in range(self.num_iterations):\n",
    "#             linear_model = np.dot(X, self.weights) + self.bias\n",
    "#             y_predicted = self.sigmoid(linear_model)\n",
    "            \n",
    "#             dw = (1 / self.m) * np.dot(X.T, (y_predicted - y)) + (self.regularization_strength / self.m) * self.weights\n",
    "#             db = (1 / self.m) * np.sum(y_predicted - y)\n",
    "            \n",
    "#             self.weights -= self.learning_rate * dw\n",
    "#             self.bias -= self.learning_rate * db\n",
    "    \n",
    "#     def predict(self, X):\n",
    "#         linear_model = np.dot(X, self.weights) + self.bias\n",
    "#         y_predicted = self.sigmoid(linear_model)\n",
    "#         y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "#         return y_predicted_cls\n",
    "    \n",
    "#     def accuracy(self, y_true, y_pred):\n",
    "#         accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "#         return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR , Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, regularization_strength=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.regularization_strength = regularization_strength  # L2 regularization strength\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        # Clip values to avoid overflow\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.m, self.n = X.shape\n",
    "        self.weights = np.zeros(self.n)\n",
    "        self.bias = 0\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        for i in range(self.num_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "            \n",
    "            dw = (1 / self.m) * np.dot(X.T, (y_predicted - y)) + (self.regularization_strength / self.m) * self.weights\n",
    "            db = (1 / self.m) * np.sum(y_predicted - y)\n",
    "            \n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
    "        return y_predicted_cls\n",
    "    \n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "    \n",
    "    def compute_loss(self, X, y):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        # Clip predicted probabilities to avoid log(0)\n",
    "        y_predicted = np.clip(y_predicted, 1e-15, 1 - 1e-15)\n",
    "        loss = -1 / self.m * (np.dot(y, np.log(y_predicted)) + np.dot((1 - y), np.log(1 - y_predicted)))\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging_logistic_regression(df, target_column, n_estimators=9, learning_rate=0.01, l2_penalty=0.01, random_state=42, test_size=0.2,iterations=1000):\n",
    "    \"\"\"\n",
    "    Perform bagging with logistic regression on the given dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame, the input dataset.\n",
    "    - target_column: str, the name of the target column.\n",
    "    - n_estimators: int, the number of bootstrap samples and models.\n",
    "    - learning_rate: float, the learning rate for logistic regression.\n",
    "    - l2_penalty: float, the L2 regularization strength.\n",
    "    \n",
    "    Returns:\n",
    "    - models: list, trained logistic regression models.\n",
    "    - X_test: DataFrame, the test set features.\n",
    "    - y_test: Series, the test set target.\n",
    "    - y_pred: ndarray, aggregated predictions from all models.\n",
    "    - accuracies: list, accuracies of each model.\n",
    "    - losses: list, losses of each model.\n",
    "    \"\"\"\n",
    "    # Split the dataset into features and target\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    models = []\n",
    "    predictions = []\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(n_estimators):\n",
    "        # Generate a bootstrap sample\n",
    "        X_resampled, y_resampled = resample(X_train, y_train, replace=True, random_state=i)\n",
    "        \n",
    "        # Create and train a logistic regression model\n",
    "        model = LogisticRegression(learning_rate=learning_rate, num_iterations=iterations, regularization_strength=l2_penalty)\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Store the model\n",
    "        models.append(model)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        predictions.append(y_pred)\n",
    "        \n",
    "        # Calculate accuracy and loss\n",
    "        accuracy = model.accuracy(y_test, y_pred)\n",
    "        loss = model.compute_loss(X_test, y_test)\n",
    "        \n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Print accuracy and loss\n",
    "        # print(f\"Model {i+1} - Accuracy: {accuracy}, Loss: {loss}\")\n",
    "    \n",
    "    # Aggregate predictions by majority vote\n",
    "    predictions = np.array(predictions)\n",
    "    y_pred_final = np.round(np.mean(predictions, axis=0)).astype(int)\n",
    "    \n",
    "    # Calculate and print the final accuracy after majority voting\n",
    "    final_accuracy = accuracy_score(y_test, y_pred_final)\n",
    "    print(f\"Final Accuracy after Majority Voting: {final_accuracy}\")\n",
    "    \n",
    "    # # Evaluate the model\n",
    "    # print(confusion_matrix(y_test, y_pred_final))\n",
    "    # print(classification_report(y_test, y_pred_final))\n",
    "    \n",
    "    return models, X_test, y_test, y_pred_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violine Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def generate_violin_plot(df, target_column, n_estimators=9, learning_rate=0.01, l2_penalty=0.01, num_iterations=1000):\n",
    "#     \"\"\"\n",
    "#     Generate a violin plot for the predictions of a bagging ensemble of logistic regression models.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - df: DataFrame, the input dataset.\n",
    "#     - target_column: str, the name of the target column.\n",
    "#     - n_estimators: int, the number of bootstrap samples and models.\n",
    "#     - learning_rate: float, the learning rate for logistic regression.\n",
    "#     - l2_penalty: float, the L2 regularization strength.\n",
    "#     - num_iterations: int, the number of iterations for logistic regression.\n",
    "#     \"\"\"\n",
    "#     # Perform bagging logistic regression\n",
    "#     models, X_test, y_test, y_pred_final, predictions = bagging_logistic_regression(df, target_column, n_estimators, learning_rate, l2_penalty, num_iterations)\n",
    "    \n",
    "#     # Convert predictions to DataFrame for plotting\n",
    "#     predictions_df = pd.DataFrame(predictions.T, columns=[f'Model_{i+1}' for i in range(n_estimators)])\n",
    "#     predictions_df['True_Label'] = y_test.values\n",
    "    \n",
    "#     # Generate violin plot\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     sns.violinplot(data=predictions_df.drop('True_Label', axis=1))\n",
    "#     plt.title('Violin Plot of Predictions from Bagging Logistic Regression Models')\n",
    "#     plt.xlabel('Models')\n",
    "#     plt.ylabel('Predicted Probability')\n",
    "#     plt.show()\n",
    "\n",
    "# # Example usage\n",
    "# generate_violin_plot(df_first_dataset_final, target_column='income', n_estimators=9, learning_rate=0.01, l2_penalty=0.01, num_iterations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix, log_loss\n",
    "\n",
    "def calculate_metrics(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate and print the accuracy, sensitivity, specificity, precision, F1-score, AUROC, and AUPR for the given model and test data.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: trained logistic regression model.\n",
    "    - X_test: DataFrame, the test set features.\n",
    "    - y_test: Series, the test set target.\n",
    "    \n",
    "    Returns:\n",
    "    - metrics: dict, containing accuracy, loss, sensitivity, specificity, precision, F1-score, AUROC, and AUPR.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    loss = log_loss(y_test, y_pred)\n",
    "    sensitivity = recall_score(y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auroc = roc_auc_score(y_test, y_pred)\n",
    "    aupr = average_precision_score(y_test, y_pred)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Sensitivity: {sensitivity}\")\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"F1-score: {f1}\")\n",
    "    print(f\"AUROC: {auroc}\")\n",
    "    print(f\"AUPR: {aupr}\")\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'loss': loss,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'precision': precision,\n",
    "        'f1_score': f1,\n",
    "        'auroc': auroc,\n",
    "        'aupr': aupr\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have a trained model and test data\n",
    "# model, X_test, y_test = ... (from your previous code)\n",
    "# metrics = calculate_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_models(df, target_column, learning_rate_lr=0.01, l2_penalty_for_lr=0.01, iterations_lr = 1000,learning_rate_bagging=0.01, l2_penalty_bagging=0.01, n_estimators=9,iterations_bagging=1000,test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Train and evaluate both a single logistic regression model and a bagging ensemble of logistic regression models on the given dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame, the input dataset.\n",
    "    - target_column: str, the name of the target column.\n",
    "    - learning_rate: float, the learning rate for logistic regression.\n",
    "    - l2_penalty: float, the L2 regularization strength.\n",
    "    \n",
    "    Returns:\n",
    "    - metrics_single: dict, metrics for the single logistic regression model.\n",
    "    - metrics_bagging: dict, metrics for the bagging ensemble of logistic regression models.\n",
    "    \"\"\"\n",
    "    # Split the dataset into features and target\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Train and evaluate a single logistic regression model\n",
    "    print(\"Metrics for Single Logistic Regression Model:\")\n",
    "    single_model = LogisticRegression(learning_rate=learning_rate_lr, num_iterations=iterations_lr, regularization_strength=l2_penalty_for_lr)\n",
    "    single_model.fit(X_train, y_train)\n",
    "    y_pred_single = single_model.predict(X_test)\n",
    "    metrics_single = calculate_metrics(y_test, y_pred_single)\n",
    "    \n",
    "    # Train and evaluate a bagging ensemble of logistic regression models\n",
    "    print(\"\\nMetrics for Bagging Logistic Regression Model:\")\n",
    "    models, X_test, y_test, y_pred_final = bagging_logistic_regression(df, target_column, n_estimators=n_estimators, learning_rate=learning_rate_bagging, l2_penalty=l2_penalty_bagging)\n",
    "    metrics_bagging = calculate_metrics(y_test, y_pred_final)\n",
    "\n",
    "    # print(\"\\nMetrics for Single Logistic Regression Model:\")\n",
    "    # print(metrics_single)\n",
    "    # print(\"\\nMetrics for Bagging Logistic Regression Model:\")\n",
    "    # print(metrics_bagging)\n",
    "    \n",
    "    return metrics_single, metrics_bagging\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "evaluate_models(df_first_dataset_final, target_column='income', learning_rate_lr=0.01, l2_penalty_for_lr=0.01, iterations_lr = 1000,learning_rate_bagging=0.01, l2_penalty_bagging=0.01, n_estimators=9,iterations_bagging=1000,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models(df_second_dataset_final, target_column='Churn', learning_rate_lr=0.01, l2_penalty_for_lr=0.01, iterations_lr = 1000,learning_rate_bagging=0.01, l2_penalty_bagging=0.01, n_estimators=9,iterations_bagging=1000,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models(df_third_dataset_final, target_column='Class', learning_rate_lr=0.01, l2_penalty_for_lr=0.01, iterations_lr = 1000,learning_rate_bagging=0.01, l2_penalty_bagging=0.01, n_estimators=9,iterations_bagging=1000,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Vs Learning Rate Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_accuracy_vs_learning_rate(dataset, feature_name, learning_rates):\n",
    "    \"\"\"\n",
    "    Plots the accuracy vs learning rate for Logistic Regression using the custom class.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: DataFrame, the input dataset.\n",
    "    - feature_name: str, the name of the target feature.\n",
    "    - learning_rates: list, a list of learning rates to evaluate.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract features and target variable\n",
    "    X = dataset.drop(columns=[feature_name])\n",
    "    y = dataset[feature_name]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        # Initialize the custom logistic regression model\n",
    "        custom_log_reg = LogisticRegression(learning_rate=lr, num_iterations=1000)\n",
    "\n",
    "        # Train the model\n",
    "        custom_log_reg.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred_custom = custom_log_reg.predict(X_test)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy_custom = custom_log_reg.accuracy(y_test, y_pred_custom)\n",
    "        accuracies.append(accuracy_custom)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(learning_rates, accuracies, marker='o')\n",
    "    plt.title('Accuracy vs Learning Rate')\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_ensemble(df, target_column, learning_rate=0.01, num_iterations=1000, random_state=42,l2_penalty=0.01):\n",
    "    \"\"\"\n",
    "    Implement stacking ensemble with logistic regression models.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame, the input dataset.\n",
    "    - target_column: str, the name of the target column.\n",
    "    - learning_rate: float, the learning rate for logistic regression.\n",
    "    - num_iterations: int, the number of iterations for logistic regression.\n",
    "    - random_state: int, the random state for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - X_test: DataFrame, the test set features.\n",
    "    - y_test: Series, the test set target.\n",
    "    - y_pred_final: ndarray, predictions from the stacking ensemble.\n",
    "    \"\"\"\n",
    "    # Split the dataset into features and target\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    # Further split the training set into training and validation sets\n",
    "    X_train_main, X_val, y_train_main, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    base_learners = []\n",
    "    meta_features = np.zeros((X_val.shape[0], 9))\n",
    "    \n",
    "    for i in range(9):\n",
    "        # Generate a bootstrap sample\n",
    "        X_resampled, y_resampled = resample(X_train_main, y_train_main, replace=True, random_state=i)\n",
    "        \n",
    "        # Create and train a logistic regression model\n",
    "        model = LogisticRegression(learning_rate=learning_rate, num_iterations=num_iterations, regularization_strength=l2_penalty)\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        \n",
    "        # Store the base learner\n",
    "        base_learners.append(model)\n",
    "        \n",
    "        # Generate meta features using the validation set\n",
    "        meta_features[:, i] = model.predict(X_val)\n",
    "    \n",
    "    # Train the meta classifier\n",
    "    meta_classifier = LogisticRegression(learning_rate=learning_rate, num_iterations=num_iterations, regularization_strength=l2_penalty)\n",
    "    meta_classifier.fit(meta_features, y_val)\n",
    "    \n",
    "    # Generate meta features for the test set\n",
    "    meta_features_test = np.zeros((X_test.shape[0], 9))\n",
    "    for i, model in enumerate(base_learners):\n",
    "        meta_features_test[:, i] = model.predict(X_test)\n",
    "    \n",
    "    # Make final predictions using the meta classifier\n",
    "    y_pred_final = meta_classifier.predict(meta_features_test)\n",
    "    \n",
    "    return X_test, y_test, y_pred_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8392088542734167\n",
      "Loss: 5.7955003246079935\n",
      "Sensitivity: 0.596989966555184\n",
      "Specificity: 0.9178658702144991\n",
      "Precision: 0.7024102311854402\n",
      "F1-score: 0.6454237288135594\n",
      "AUROC: 0.7574279183848416\n",
      "AUPR: 0.5181225962299327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8392088542734167,\n",
       " 'loss': 5.7955003246079935,\n",
       " 'sensitivity': 0.596989966555184,\n",
       " 'specificity': 0.9178658702144991,\n",
       " 'precision': 0.7024102311854402,\n",
       " 'f1_score': 0.6454237288135594,\n",
       " 'auroc': 0.7574279183848416,\n",
       " 'aupr': 0.5181225962299327}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "X_test, y_test, y_pred_final = stacking_ensemble(df_first_dataset_final, target_column='income', learning_rate=0.01, num_iterations=1000, random_state=42)\n",
    "\n",
    "calculate_metrics(y_test, y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7594306049822064\n",
      "Loss: 8.67099989005096\n",
      "Sensitivity: 0.26988636363636365\n",
      "Specificity: 0.9230769230769231\n",
      "Precision: 0.5397727272727273\n",
      "F1-score: 0.35984848484848486\n",
      "AUROC: 0.5964816433566434\n",
      "AUPR: 0.3285954480199112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7594306049822064,\n",
       " 'loss': 8.67099989005096,\n",
       " 'sensitivity': 0.26988636363636365,\n",
       " 'specificity': 0.9230769230769231,\n",
       " 'precision': 0.5397727272727273,\n",
       " 'f1_score': 0.35984848484848486,\n",
       " 'auroc': 0.5964816433566434,\n",
       " 'aupr': 0.3285954480199112}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test, y_pred_final = stacking_ensemble(df_second_dataset_final, target_column='Churn', learning_rate=0.01, num_iterations=1000, random_state=42)\n",
    "\n",
    "calculate_metrics(y_test, y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    283253\n",
       "1       473\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_third_dataset_final['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20473, 31)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate positive and negative samples\n",
    "positive_samples = df_third_dataset_final[df_third_dataset_final['Class'] == 1]\n",
    "negative_samples = df_third_dataset_final[df_third_dataset_final['Class'] == 0]\n",
    "\n",
    "# Randomly select 20,000 negative samples\n",
    "negative_samples_selected = negative_samples.sample(n=20000, random_state=42)\n",
    "\n",
    "# Combine the positive samples with the selected negative samples\n",
    "df_sampled = pd.concat([positive_samples, negative_samples_selected])\n",
    "\n",
    "# Shuffle the combined dataframe\n",
    "df_sampled = df_sampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9938949938949939\n",
      "Loss: 0.22004672398728442\n",
      "Sensitivity: 0.7967479674796748\n",
      "Specificity: 1.0\n",
      "Precision: 1.0\n",
      "F1-score: 0.8868778280542986\n",
      "AUROC: 0.8983739837398375\n",
      "AUPR: 0.8028529735846809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9938949938949939,\n",
       " 'loss': 0.22004672398728442,\n",
       " 'sensitivity': 0.7967479674796748,\n",
       " 'specificity': 1.0,\n",
       " 'precision': 1.0,\n",
       " 'f1_score': 0.8868778280542986,\n",
       " 'auroc': 0.8983739837398375,\n",
       " 'aupr': 0.8028529735846809}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test, y_test, y_pred_final = stacking_ensemble(df_sampled, target_column='Class', learning_rate=0.5, num_iterations=1000, random_state=42,l2_penalty=0.0005)\n",
    "\n",
    "calculate_metrics(y_test, y_pred_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
