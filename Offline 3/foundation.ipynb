{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from scipy.special import softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated data loading function with train-validation split\n",
    "def load_data(validation_split=0.2):\n",
    "    # Load training and test data\n",
    "    train_data = FashionMNIST(root=\".\", train=True, download=True, transform=ToTensor())\n",
    "    test_data = FashionMNIST(root=\".\", train=False, download=True, transform=ToTensor())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train = np.array([np.array(img[0]).flatten() for img in train_data])\n",
    "    y_train = np.array(train_data.targets)\n",
    "    X_test = np.array([np.array(img[0]).flatten() for img in test_data])\n",
    "    y_test = np.array(test_data.targets)\n",
    "    \n",
    "    # Split training data into train and validation sets\n",
    "    num_train_samples = int((1 - validation_split) * X_train.shape[0])\n",
    "    indices = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    X_train, X_val = X_train[indices[:num_train_samples]], X_train[indices[num_train_samples:]]\n",
    "    y_train, y_val = y_train[indices[:num_train_samples]], y_train[indices[num_train_samples:]]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Load the data with 20% validation split\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Store input for use in backpropagation\n",
    "        self.X = X\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        # Compute gradients\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(self.X.T, grad_output)\n",
    "        grad_bias = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.bias -= learning_rate * grad_bias\n",
    "        \n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, num_features, epsilon=1e-5, momentum=0.9):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.gamma = np.ones((1, num_features))\n",
    "        self.beta = np.zeros((1, num_features))\n",
    "        self.running_mean = np.zeros((1, num_features))\n",
    "        self.running_var = np.ones((1, num_features))\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            batch_mean = np.mean(X, axis=0)\n",
    "            batch_var = np.var(X, axis=0)\n",
    "            self.X_centered = X - batch_mean\n",
    "            self.stddev_inv = 1. / np.sqrt(batch_var + self.epsilon)\n",
    "            \n",
    "            # Normalize\n",
    "            self.X_norm = self.X_centered * self.stddev_inv\n",
    "            self.out = self.gamma * self.X_norm + self.beta\n",
    "            \n",
    "            # Update running statistics\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "        else:\n",
    "            # Use running statistics for inference\n",
    "            self.X_norm = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            self.out = self.gamma * self.X_norm + self.beta\n",
    "            \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_gamma = np.sum(grad_output * self.X_norm, axis=0)\n",
    "        grad_beta = np.sum(grad_output, axis=0)\n",
    "        \n",
    "        grad_X_norm = grad_output * self.gamma\n",
    "        grad_var = np.sum(grad_X_norm * self.X_centered, axis=0) * -0.5 * self.stddev_inv**3\n",
    "        grad_mean = np.sum(grad_X_norm * -self.stddev_inv, axis=0) + grad_var * np.mean(-2. * self.X_centered, axis=0)\n",
    "        \n",
    "        grad_input = grad_X_norm * self.stddev_inv + grad_var * 2 * self.X_centered / grad_output.shape[0] + grad_mean / grad_output.shape[0]\n",
    "        \n",
    "        # Update gamma and beta\n",
    "        self.gamma -= learning_rate * grad_gamma\n",
    "        self.beta -= learning_rate * grad_beta\n",
    "        \n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        return grad_output * (self.X > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None  # Initialize mask as None\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            # Apply dropout mask during training\n",
    "            # print(\"in dropout\")\n",
    "            self.mask = (np.random.rand(*X.shape) > self.dropout_rate) / (1 - self.dropout_rate)\n",
    "            # if(self.mask is None):\n",
    "            #     print(\"mask is none\")\n",
    "            # else :\n",
    "            #     print(\"mask is not none\")\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            # No dropout during inference; mask is effectively all 1s\n",
    "            self.mask = np.ones_like(X)\n",
    "            return X\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate=None):\n",
    "        # Use the mask for backpropagation\n",
    "        if self.mask is None:\n",
    "            raise ValueError(\"Mask has not been initialized. Ensure forward pass is called before backward.\")\n",
    "        return grad_output * self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, layer):\n",
    "        if layer not in self.m:\n",
    "            # Initialize first and second moment vectors\n",
    "            self.m[layer] = np.zeros_like(layer.weights)\n",
    "            self.v[layer] = np.zeros_like(layer.weights)\n",
    "\n",
    "        # Update time step\n",
    "        self.t += 1\n",
    "\n",
    "        # Calculate gradients\n",
    "        grad_weights = layer.grad_weights\n",
    "        grad_bias = layer.grad_bias\n",
    "\n",
    "        # Update biased first moment estimate\n",
    "        self.m[layer] = self.beta1 * self.m[layer] + (1 - self.beta1) * grad_weights\n",
    "        self.v[layer] = self.beta2 * self.v[layer] + (1 - self.beta2) * (grad_weights**2)\n",
    "\n",
    "        # Bias-corrected first and second moment estimates\n",
    "        m_hat = self.m[layer] / (1 - self.beta1**self.t)\n",
    "        v_hat = self.v[layer] / (1 - self.beta2**self.t)\n",
    "\n",
    "        # Update weights and biases\n",
    "        layer.weights -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        layer.bias -= self.learning_rate * grad_bias  # No moment estimates for biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, X):\n",
    "        # Apply softmax activation\n",
    "        exp_values = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate=None):\n",
    "        # Gradient for the softmax layer (assuming Cross-Entropy Loss)\n",
    "        return grad_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, dropout_rate=0.5):\n",
    "        self.layers = layers\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        # Forward pass through all layers\n",
    "        for layer in self.layers:\n",
    "            # X = layer.forward(X) if not isinstance(layer, Dropout) else self.dropout.forward(X, training)\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        # Backward pass through all layers in reverse order\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "def evaluate(network, X_test, y_test):\n",
    "    # Forward pass through the test data\n",
    "    predictions = network.forward(X_test, training=False)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate accuracy and F1 score\n",
    "    accuracy = accuracy_score(y_test, predicted_labels)\n",
    "    f1 = f1_score(y_test, predicted_labels, average=\"macro\")\n",
    "    conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "    \n",
    "    return accuracy, f1, conf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model using pickle\n",
    "def save_model(network, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(network, f)\n",
    "\n",
    "# Load model from pickle file\n",
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute loss (cross-entropy)\n",
    "def compute_loss(predictions, targets):\n",
    "    # One-hot encode targets\n",
    "    targets_one_hot = np.eye(10)[targets]\n",
    "    # Calculate loss\n",
    "    loss = -np.mean(np.sum(targets_one_hot * np.log(predictions + 1e-9), axis=1))\n",
    "    return loss\n",
    "\n",
    "# Updated training function to report additional metrics\n",
    "def train(network, X_train, y_train, X_val, y_val, epochs, batch_size, learning_rate, optimizer):\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data\n",
    "        indices = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "        \n",
    "        # Training in mini-batches\n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            X_batch = X_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = network.forward(X_batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = compute_loss(predictions, y_batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            grad_output = predictions - np.eye(10)[y_batch]\n",
    "            network.backward(grad_output, learning_rate)\n",
    "        \n",
    "        # Validate after each epoch\n",
    "        val_predictions = network.forward(X_val, training=False)\n",
    "        val_loss = compute_loss(val_predictions, y_val)\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        train_predictions = network.forward(X_train, training=False)\n",
    "        train_acc = accuracy_score(y_train, np.argmax(train_predictions, axis=1))\n",
    "        \n",
    "        # Calculate validation accuracy and F1 score\n",
    "        val_acc = accuracy_score(y_val, np.argmax(val_predictions, axis=1))\n",
    "        val_f1 = f1_score(y_val, np.argmax(val_predictions, axis=1), average='macro')\n",
    "        \n",
    "        # Record metrics\n",
    "        history['train_loss'].append(loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}, Training Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}, Validation Macro-F1: {val_f1:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to plot graphs for different learning rates and models\n",
    "# def plot_metrics(histories, learning_rates, model_names):\n",
    "#     for i, history in enumerate(histories):\n",
    "#         plt.figure(figsize=(12, 8))\n",
    "#         plt.subplot(2, 2, 1)\n",
    "#         plt.plot(history['train_loss'], label='Train Loss')\n",
    "#         plt.plot(history['val_loss'], label='Validation Loss')\n",
    "#         plt.title(f'{model_names[i]} - Loss')\n",
    "#         plt.legend()\n",
    "        \n",
    "#         plt.subplot(2, 2, 2)\n",
    "#         plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "#         plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "#         plt.title(f'{model_names[i]} - Accuracy')\n",
    "#         plt.legend()\n",
    "        \n",
    "#         plt.subplot(2, 2, 3)\n",
    "#         plt.plot(history['val_f1'], label='Validation Macro-F1')\n",
    "#         plt.title(f'{model_names[i]} - Validation Macro-F1')\n",
    "#         plt.legend()\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training function that saves the best model based on validation macro-F1 score\n",
    "def train_and_save_best_model(network, X_train, y_train, X_val, y_val, epochs, batch_size, learning_rate, optimizer, save_path='best_model.pickle'):\n",
    "    # Lists to store metrics\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_accuracy': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_macro_f1': []\n",
    "    }\n",
    "    best_f1 = 0  # Track best validation macro-F1 score\n",
    "    best_model_path = save_path  # Track path of the best model\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Adjust learning rate every 3 epochs\n",
    "        if epoch > 0 and epoch % 3 == 0:\n",
    "            learning_rate *= 0.5\n",
    "\n",
    "        # Shuffle training data\n",
    "        indices = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train, y_train = X_train[indices], y_train[indices]\n",
    "        \n",
    "        # Training in mini-batches\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            X_batch = X_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = network.forward(X_batch)\n",
    "            loss = compute_loss(predictions, y_batch)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Backward pass\n",
    "            grad_output = predictions - np.eye(10)[y_batch]\n",
    "            network.backward(grad_output, learning_rate)\n",
    "        \n",
    "        # Calculate and store average training loss\n",
    "        avg_train_loss = epoch_loss / (X_train.shape[0] // batch_size)\n",
    "        \n",
    "        # Validation pass\n",
    "        val_predictions = network.forward(X_val, training=False)\n",
    "        val_loss = compute_loss(val_predictions, y_val)\n",
    "        \n",
    "        # Calculate training and validation accuracy\n",
    "        train_accuracy = accuracy_score(y_train, np.argmax(network.forward(X_train, training=False), axis=1))\n",
    "        val_accuracy = accuracy_score(y_val, np.argmax(val_predictions, axis=1))\n",
    "        \n",
    "        # Calculate validation macro-F1 score\n",
    "        val_macro_f1 = f1_score(y_val, np.argmax(val_predictions, axis=1), average=\"macro\")\n",
    "        \n",
    "        # Append metrics to history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_accuracy'].append(train_accuracy)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        history['val_macro_f1'].append(val_macro_f1)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}, \"\n",
    "              f\"Val Macro-F1: {val_macro_f1:.4f}\")\n",
    "        \n",
    "        # Save the model if it has the best validation macro-F1 so far\n",
    "        if val_macro_f1 > best_f1:\n",
    "            best_f1 = val_macro_f1\n",
    "            best_model_path = save_path\n",
    "            save_model(network, best_model_path)\n",
    "            print(f\"New best model saved with Validation Macro-F1: {val_macro_f1:.4f}\")\n",
    "    \n",
    "    return history, best_model_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot metrics\n",
    "def plot_metric(history, metric, title):\n",
    "    for (lr, arch), hist in history.items():\n",
    "        plt.plot(hist[metric], label=f\"LR: {lr}, Arch: {arch}\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric.replace('_', ' ').title())\n",
    "    plt.legend()\n",
    "    # plt.show()\n",
    "    plt.savefig(f\"Figures/{metric}.png\")\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define architectures\n",
    "architectures = [\n",
    "    [DenseLayer(784, 128), ReLU(), DenseLayer(128, 10), Softmax()],\n",
    "    [DenseLayer(784, 256), ReLU(), DenseLayer(256, 128), ReLU(), DenseLayer(128, 10), Softmax()],\n",
    "    [DenseLayer(784, 512), ReLU(), BatchNormalization(512), DenseLayer(512, 256), ReLU(), DenseLayer(256, 10), Softmax()]\n",
    "]\n",
    "\n",
    "# Test different learning rates\n",
    "learning_rates = [0.005, 0.0025, 0.001, 0.0005]\n",
    "\n",
    "# Dictionary to store results and best model paths\n",
    "results = {}\n",
    "best_model_paths = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for i, arch in enumerate(architectures):\n",
    "        print(f\"\\nTraining model with architecture {i+1} and learning rate {lr}\")\n",
    "        network = NeuralNetwork(arch)\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        history, best_model_path = train_and_save_best_model(\n",
    "            network, X_train, y_train, X_val, y_val, epochs=10, batch_size=64, learning_rate=lr, \n",
    "            optimizer=optimizer, save_path=f'best_model_arch{i+1}_lr{lr}.pickle'\n",
    "        )\n",
    "        \n",
    "        # Save history and best model path for reporting\n",
    "        results[(lr, f\"arch_{i+1}\")] = history\n",
    "        best_model_paths[(lr, f\"arch_{i+1}\")] = best_model_path\n",
    "\n",
    "# Plot metrics for all architectures and learning rates\n",
    "plot_metric(results, 'train_loss', 'Training Loss over Epochs')\n",
    "plot_metric(results, 'val_loss', 'Validation Loss over Epochs')\n",
    "plot_metric(results, 'train_accuracy', 'Training Accuracy over Epochs')\n",
    "plot_metric(results, 'val_accuracy', 'Validation Accuracy over Epochs')\n",
    "plot_metric(results, 'val_macro_f1', 'Validation Macro F1 Score over Epochs')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on the highest validation macro-F1 score\n",
    "best_score = 0\n",
    "best_config = None\n",
    "\n",
    "for config, hist in results.items():\n",
    "    max_val_f1 = max(hist['val_macro_f1'])\n",
    "    if max_val_f1 > best_score:\n",
    "        best_score = max_val_f1\n",
    "        best_config = config\n",
    "\n",
    "print(f\"Best Model Configuration: {best_config}\")\n",
    "print(f\"Best Validation Macro-F1 Score: {best_score:.4f}\")\n",
    "\n",
    "# Load and evaluate the best model on the test set\n",
    "best_model_path = best_model_paths[best_config]\n",
    "best_network = load_model(best_model_path)\n",
    "\n",
    "# Test set evaluation\n",
    "test_predictions = best_network.forward(X_test, training=False)\n",
    "test_accuracy = accuracy_score(y_test, np.argmax(test_predictions, axis=1))\n",
    "test_macro_f1 = f1_score(y_test, np.argmax(test_predictions, axis=1), average=\"macro\")\n",
    "test_conf_matrix = confusion_matrix(y_test, np.argmax(test_predictions, axis=1))\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Macro F1 Score: {test_macro_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", test_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define network layers\n",
    "# input_dim = 28 * 28\n",
    "# output_dim = 10\n",
    "\n",
    "# layers = [\n",
    "#     DenseLayer(input_dim, 128),\n",
    "#     ReLU(),\n",
    "#     BatchNormalization(128),\n",
    "#     Dropout(0.5),\n",
    "#     DenseLayer(128, 64),\n",
    "#     ReLU(),\n",
    "#     BatchNormalization(64),\n",
    "#     Dropout(0.5),\n",
    "#     DenseLayer(64, output_dim),\n",
    "#     Softmax()\n",
    "# ]\n",
    "\n",
    "# # Initialize the neural network and optimizer\n",
    "# network = NeuralNetwork(layers)\n",
    "# optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "# # Train the model\n",
    "# train(network, X_train, y_train, X_val, y_val, epochs=10, batch_size=64, learning_rate=0.001, optimizer=optimizer)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy, f1, conf_matrix = evaluate(network, X_test, y_test)\n",
    "\n",
    "# # Save the trained model\n",
    "# save_model(network, 'model_yourRollNo.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Define and train models with different learning rates\n",
    "# learning_rates = [0.005, 0.001, 0.0005, 0.0001]\n",
    "# model_names = ['Model 1', 'Model 2', 'Model 3']\n",
    "# histories = []\n",
    "\n",
    "# for lr in learning_rates:\n",
    "#     for model_name in model_names:\n",
    "#         # Define network layers\n",
    "#         layers = [\n",
    "#             DenseLayer(input_dim, 128),\n",
    "#             ReLU(),\n",
    "#             BatchNormalization(128),\n",
    "#             Dropout(0.5),\n",
    "#             DenseLayer(128, 64),\n",
    "#             ReLU(),\n",
    "#             BatchNormalization(64),\n",
    "#             Dropout(0.5),\n",
    "#             DenseLayer(64, output_dim),\n",
    "#             Softmax()\n",
    "#         ]\n",
    "        \n",
    "#         # Initialize the neural network and optimizer\n",
    "#         network = NeuralNetwork(layers)\n",
    "#         optimizer = Adam(learning_rate=lr)\n",
    "        \n",
    "#         # Train the model\n",
    "#         history = train(network, X_train, y_train, X_val, y_val, epochs=10, batch_size=64, learning_rate=lr, optimizer=optimizer)\n",
    "#         histories.append(history)\n",
    "        \n",
    "#         # Evaluate the model\n",
    "#         accuracy, f1, conf_matrix = evaluate(network, X_test, y_test)\n",
    "#         print(f\"Learning Rate: {lr}, Model: {model_name}, Test Accuracy: {accuracy:.4f}, Macro F1 Score: {f1:.4f}\")\n",
    "#         print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# # Plot metrics for all models and learning rates\n",
    "# plot_metrics(histories, learning_rates, model_names)\n",
    "\n",
    "# # Select the best model based on validation macro-f1 score\n",
    "# best_model_index = np.argmax([max(history['val_f1']) for history in histories])\n",
    "# best_model = histories[best_model_index]\n",
    "\n",
    "# # Report the independent test performance for the best model\n",
    "# best_accuracy, best_f1, best_conf_matrix = evaluate(network, X_test, y_test)\n",
    "# print(f\"Best Model Test Accuracy: {best_accuracy:.4f}, Macro F1 Score: {best_f1:.4f}\")\n",
    "# print(\"Best Model Confusion Matrix:\\n\", best_conf_matrix)\n",
    "\n",
    "# # Save the best model\n",
    "# save_model(network, 'best_model.pickle')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
