{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, confusion_matrix,accuracy_score\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated data loading function with train-validation split\n",
    "def load_data(validation_split=0.2):\n",
    "    # Load training and test data\n",
    "    train_data = FashionMNIST(root=\".\", train=True, download=True, transform=ToTensor())\n",
    "    test_data = FashionMNIST(root=\".\", train=False, download=True, transform=ToTensor())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train = np.array([np.array(img[0]).flatten() for img in train_data])\n",
    "    y_train = np.array(train_data.targets)\n",
    "    X_test = np.array([np.array(img[0]).flatten() for img in test_data])\n",
    "    y_test = np.array(test_data.targets)\n",
    "    \n",
    "    # Split training data into train and validation sets\n",
    "    num_train_samples = int((1 - validation_split) * X_train.shape[0])\n",
    "    indices = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    X_train, X_val = X_train[indices[:num_train_samples]], X_train[indices[num_train_samples:]]\n",
    "    y_train, y_val = y_train[indices[:num_train_samples]], y_train[indices[num_train_samples:]]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Load the data with 20% validation split\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialize weights and biases as float32 to reduce size\n",
    "        self.weights = np.random.randn(input_dim, output_dim).astype(np.float32) * 0.01\n",
    "        self.bias = np.zeros((1, output_dim), dtype=np.float32)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X  # Store input for backpropagation\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(self.X.T, grad_output)\n",
    "        grad_bias = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * grad_weights.astype(np.float32)\n",
    "        self.bias -= learning_rate * grad_bias.astype(np.float32)\n",
    "        \n",
    "        return grad_input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, num_features, epsilon=1e-5, momentum=0.9):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.gamma = np.ones((1, num_features), dtype=np.float32)\n",
    "        self.beta = np.zeros((1, num_features), dtype=np.float32)\n",
    "        self.running_mean = np.zeros((1, num_features), dtype=np.float32)\n",
    "        self.running_var = np.ones((1, num_features), dtype=np.float32)\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            batch_mean = np.mean(X, axis=0).astype(np.float32)\n",
    "            batch_var = np.var(X, axis=0).astype(np.float32)\n",
    "            self.X_centered = (X - batch_mean).astype(np.float32)\n",
    "            self.stddev_inv = (1. / np.sqrt(batch_var + self.epsilon)).astype(np.float32)\n",
    "            \n",
    "            # Normalize\n",
    "            self.X_norm = self.X_centered * self.stddev_inv\n",
    "            self.out = self.gamma * self.X_norm + self.beta\n",
    "            \n",
    "            # Update running statistics\n",
    "            self.running_mean = (self.momentum * self.running_mean + (1 - self.momentum) * batch_mean).astype(np.float32)\n",
    "            self.running_var = (self.momentum * self.running_var + (1 - self.momentum) * batch_var).astype(np.float32)\n",
    "        else:\n",
    "            # Use running statistics for inference\n",
    "            self.X_norm = (X - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            self.out = self.gamma * self.X_norm + self.beta\n",
    "            \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_gamma = np.sum(grad_output * self.X_norm, axis=0).astype(np.float32)\n",
    "        grad_beta = np.sum(grad_output, axis=0).astype(np.float32)\n",
    "        \n",
    "        grad_X_norm = grad_output * self.gamma\n",
    "        grad_var = np.sum(grad_X_norm * self.X_centered, axis=0) * -0.5 * self.stddev_inv**3\n",
    "        grad_mean = np.sum(grad_X_norm * -self.stddev_inv, axis=0) + grad_var * np.mean(-2. * self.X_centered, axis=0)\n",
    "        \n",
    "        grad_input = (grad_X_norm * self.stddev_inv + grad_var * 2 * self.X_centered / grad_output.shape[0] + grad_mean / grad_output.shape[0]).astype(np.float32)\n",
    "        \n",
    "        # Update gamma and beta\n",
    "        self.gamma -= learning_rate * grad_gamma\n",
    "        self.beta -= learning_rate * grad_beta\n",
    "        \n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        return grad_output * (self.X > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None  # Initialize mask as None\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            # Apply dropout mask during training\n",
    "            # print(\"in dropout\")\n",
    "            self.mask = (np.random.rand(*X.shape) > self.dropout_rate) / (1 - self.dropout_rate)\n",
    "            # if(self.mask is None):\n",
    "            #     print(\"mask is none\")\n",
    "            # else :\n",
    "            #     print(\"mask is not none\")\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            # No dropout during inference; mask is effectively all 1s\n",
    "            self.mask = np.ones_like(X)\n",
    "            return X\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate=None):\n",
    "        # Use the mask for backpropagation\n",
    "        if self.mask is None:\n",
    "            raise ValueError(\"Mask has not been initialized. Ensure forward pass is called before backward.\")\n",
    "        return grad_output * self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, layer):\n",
    "        if layer not in self.m:\n",
    "            # Initialize first and second moment vectors\n",
    "            self.m[layer] = np.zeros_like(layer.weights)\n",
    "            self.v[layer] = np.zeros_like(layer.weights)\n",
    "\n",
    "        # Update time step\n",
    "        self.t += 1\n",
    "\n",
    "        # Calculate gradients\n",
    "        grad_weights = layer.grad_weights\n",
    "        grad_bias = layer.grad_bias\n",
    "\n",
    "        # Update biased first moment estimate\n",
    "        self.m[layer] = self.beta1 * self.m[layer] + (1 - self.beta1) * grad_weights\n",
    "        self.v[layer] = self.beta2 * self.v[layer] + (1 - self.beta2) * (grad_weights**2)\n",
    "\n",
    "        # Bias-corrected first and second moment estimates\n",
    "        m_hat = self.m[layer] / (1 - self.beta1**self.t)\n",
    "        v_hat = self.v[layer] / (1 - self.beta2**self.t)\n",
    "\n",
    "        # Update weights and biases\n",
    "        layer.weights -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        layer.bias -= self.learning_rate * grad_bias  # No moment estimates for biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, X):\n",
    "        # Apply softmax activation\n",
    "        exp_values = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate=None):\n",
    "        # Gradient for the softmax layer (assuming Cross-Entropy Loss)\n",
    "        return grad_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output, learning_rate)\n",
    "    \n",
    "    def clean_for_saving(self):\n",
    "        # Remove unnecessary attributes from layers to reduce file size\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'X'):\n",
    "                del layer.X  # Remove input storage used in backpropagation\n",
    "            if hasattr(layer, 'X_centered'):\n",
    "                del layer.X_centered\n",
    "            if hasattr(layer, 'stddev_inv'):\n",
    "                del layer.stddev_inv\n",
    "            if hasattr(layer, 'X_norm'):\n",
    "                del layer.X_norm\n",
    "            if hasattr(layer, 'grad_weights'):\n",
    "                del layer.grad_weights\n",
    "            if hasattr(layer, 'grad_bias'):\n",
    "                del layer.grad_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(network, X_test, y_test):\n",
    "    # Forward pass through the test data\n",
    "    predictions = network.forward(X_test, training=False)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calculate accuracy and F1 score\n",
    "    accuracy = accuracy_score(y_test, predicted_labels)\n",
    "    f1 = f1_score(y_test, predicted_labels, average=\"macro\")\n",
    "    conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro F1 Score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "    \n",
    "    return accuracy, f1, conf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model using pickle\n",
    "def save_model(network, filename):\n",
    "    network.clean_for_saving()\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(network, f)\n",
    "\n",
    "# Load model from pickle file\n",
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute loss (cross-entropy)\n",
    "def compute_loss(predictions, targets):\n",
    "    # One-hot encode targets\n",
    "    targets_one_hot = np.eye(10)[targets]\n",
    "    # Calculate loss\n",
    "    loss = -np.mean(np.sum(targets_one_hot * np.log(predictions + 1e-9), axis=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the output directory exists\n",
    "os.makedirs(\"Figures\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix using Seaborn\n",
    "def plot_confusion_matrix(conf_matrix, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    # plt.show()\n",
    "    plt.savefig(f\"Figures/{title}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with added confusion matrix plotting\n",
    "def train_model(network, X_train, y_train, X_val, y_val, epochs, batch_size, learning_rate, optimizer,archnum):\n",
    "    # Lists to store metrics\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_accuracy': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_macro_f1': []\n",
    "    }\n",
    "    best_f1 = 0  # Track the best validation macro-F1 score for this run\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Adjust learning rate every 3 epochs\n",
    "        if epoch > 0 and epoch % 3 == 0:\n",
    "            learning_rate *= 0.5\n",
    "\n",
    "        # Shuffle training data\n",
    "        indices = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        X_train, y_train = X_train[indices], y_train[indices]\n",
    "        \n",
    "        # Training in mini-batches\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "            X_batch = X_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = network.forward(X_batch)\n",
    "            loss = compute_loss(predictions, y_batch)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Backward pass\n",
    "            grad_output = predictions - np.eye(10)[y_batch]\n",
    "            network.backward(grad_output, learning_rate)\n",
    "        \n",
    "        # Calculate and store average training loss\n",
    "        avg_train_loss = epoch_loss / (X_train.shape[0] // batch_size)\n",
    "        \n",
    "        # Validation pass\n",
    "        val_predictions = network.forward(X_val, training=False)\n",
    "        val_loss = compute_loss(val_predictions, y_val)\n",
    "        \n",
    "        # Calculate training and validation accuracy\n",
    "        train_preds = np.argmax(network.forward(X_train, training=False), axis=1)\n",
    "        val_preds = np.argmax(val_predictions, axis=1)\n",
    "        \n",
    "        train_accuracy = accuracy_score(y_train, train_preds)\n",
    "        val_accuracy = accuracy_score(y_val, val_preds)\n",
    "        \n",
    "        # Calculate validation macro-F1 score\n",
    "        val_macro_f1 = f1_score(y_val, val_preds, average=\"macro\")\n",
    "        \n",
    "        # Append metrics to history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_accuracy'].append(train_accuracy)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        history['val_macro_f1'].append(val_macro_f1)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}, \"\n",
    "              f\"Val Macro-F1: {val_macro_f1:.4f}\")\n",
    "        \n",
    "    # Confusion matrices for train and validation\n",
    "    train_conf_matrix = confusion_matrix(y_train, train_preds)\n",
    "    val_conf_matrix = confusion_matrix(y_val, val_preds)\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    plot_confusion_matrix(train_conf_matrix, f\"Train Confusion Matrix - Arch {archnum} - LR {learning_rate}\")\n",
    "    plot_confusion_matrix(val_conf_matrix, f\"Validation Confusion Matrix - Arch {archnum} - LR {learning_rate}\")\n",
    "\n",
    "    return history, max(history['val_macro_f1'])  # Return history and best F1 score for this model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define base colors for each architecture\n",
    "architecture_colors = sns.color_palette(\"dark\", 3)  # 3 darker colors for 3 architectures\n",
    "\n",
    "# Define line styles and markers for each learning rate\n",
    "line_styles = ['-', '--', '-.', ':']\n",
    "# markers = ['o', 's', 'D', 'x']  # Circle, square, diamond, x-mark\n",
    "\n",
    "def plot_metric(history, metric, title):\n",
    "    # Loop through each (learning rate, architecture) configuration\n",
    "    for idx, ((lr, arch), hist) in enumerate(history.items()):\n",
    "        # Determine color based on architecture and line style/marker based on learning rate\n",
    "        arch_index = int(arch.split('_')[1]) - 1  # Extract architecture index (e.g., 'arch_1' -> 0)\n",
    "        color = architecture_colors[arch_index]  # Base color for the architecture\n",
    "        line_style = line_styles[idx % len(line_styles)]  # Cycle through line styles\n",
    "        # marker = markers[idx % len(markers)]  # Cycle through markers\n",
    "        \n",
    "        # Plot the metric with chosen color, line style, and marker\n",
    "        plt.plot(hist[metric], label=f\"{arch},LR:{lr}\", color=color, linestyle=line_style)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric.replace('_', ' ').title())\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(f\"Figures/{metric}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with architecture 1 and learning rate 0.005\n",
      "Epoch 1/10 - Train Loss: 0.6449, Val Loss: 0.5009, Train Acc: 0.8148, Val Acc: 0.8107, Val Macro-F1: 0.8084\n",
      "Epoch 2/10 - Train Loss: 0.4369, Val Loss: 0.4885, Train Acc: 0.8277, Val Acc: 0.8204, Val Macro-F1: 0.8129\n",
      "Epoch 3/10 - Train Loss: 0.3906, Val Loss: 0.3989, Train Acc: 0.8622, Val Acc: 0.8502, Val Macro-F1: 0.8501\n",
      "Epoch 4/10 - Train Loss: 0.3302, Val Loss: 0.3508, Train Acc: 0.8836, Val Acc: 0.8730, Val Macro-F1: 0.8717\n",
      "Epoch 5/10 - Train Loss: 0.3156, Val Loss: 0.3404, Train Acc: 0.8919, Val Acc: 0.8739, Val Macro-F1: 0.8719\n",
      "Epoch 6/10 - Train Loss: 0.3066, Val Loss: 0.3381, Train Acc: 0.8942, Val Acc: 0.8742, Val Macro-F1: 0.8712\n",
      "Epoch 7/10 - Train Loss: 0.2801, Val Loss: 0.3251, Train Acc: 0.9021, Val Acc: 0.8818, Val Macro-F1: 0.8804\n",
      "Epoch 8/10 - Train Loss: 0.2747, Val Loss: 0.3238, Train Acc: 0.9039, Val Acc: 0.8823, Val Macro-F1: 0.8808\n",
      "Epoch 9/10 - Train Loss: 0.2706, Val Loss: 0.3185, Train Acc: 0.9049, Val Acc: 0.8835, Val Macro-F1: 0.8814\n",
      "Epoch 10/10 - Train Loss: 0.2567, Val Loss: 0.3117, Train Acc: 0.9085, Val Acc: 0.8865, Val Macro-F1: 0.8848\n",
      "New best model found: LR: 0.005, Arch: 1 with Val Macro-F1: 0.8848\n",
      "\n",
      "Training model with architecture 1 and learning rate 0.0025\n",
      "Epoch 1/10 - Train Loss: 0.2825, Val Loss: 0.3250, Train Acc: 0.9052, Val Acc: 0.8813, Val Macro-F1: 0.8787\n",
      "Epoch 2/10 - Train Loss: 0.2751, Val Loss: 0.3271, Train Acc: 0.9038, Val Acc: 0.8807, Val Macro-F1: 0.8765\n",
      "Epoch 3/10 - Train Loss: 0.2690, Val Loss: 0.3289, Train Acc: 0.9066, Val Acc: 0.8821, Val Macro-F1: 0.8795\n",
      "Epoch 4/10 - Train Loss: 0.2418, Val Loss: 0.3163, Train Acc: 0.9149, Val Acc: 0.8872, Val Macro-F1: 0.8855\n",
      "Epoch 5/10 - Train Loss: 0.2377, Val Loss: 0.3126, Train Acc: 0.9138, Val Acc: 0.8854, Val Macro-F1: 0.8851\n",
      "Epoch 6/10 - Train Loss: 0.2335, Val Loss: 0.3145, Train Acc: 0.9142, Val Acc: 0.8846, Val Macro-F1: 0.8834\n",
      "Epoch 7/10 - Train Loss: 0.2214, Val Loss: 0.3060, Train Acc: 0.9220, Val Acc: 0.8906, Val Macro-F1: 0.8894\n",
      "Epoch 8/10 - Train Loss: 0.2188, Val Loss: 0.3039, Train Acc: 0.9225, Val Acc: 0.8902, Val Macro-F1: 0.8882\n",
      "Epoch 9/10 - Train Loss: 0.2170, Val Loss: 0.3050, Train Acc: 0.9234, Val Acc: 0.8903, Val Macro-F1: 0.8889\n",
      "Epoch 10/10 - Train Loss: 0.2103, Val Loss: 0.3050, Train Acc: 0.9234, Val Acc: 0.8897, Val Macro-F1: 0.8891\n",
      "New best model found: LR: 0.0025, Arch: 1 with Val Macro-F1: 0.8894\n",
      "\n",
      "Training model with architecture 1 and learning rate 0.001\n",
      "Epoch 1/10 - Train Loss: 0.2193, Val Loss: 0.3063, Train Acc: 0.9239, Val Acc: 0.8887, Val Macro-F1: 0.8879\n",
      "Epoch 2/10 - Train Loss: 0.2177, Val Loss: 0.3059, Train Acc: 0.9236, Val Acc: 0.8896, Val Macro-F1: 0.8883\n",
      "Epoch 3/10 - Train Loss: 0.2144, Val Loss: 0.3053, Train Acc: 0.9257, Val Acc: 0.8910, Val Macro-F1: 0.8885\n",
      "Epoch 4/10 - Train Loss: 0.2031, Val Loss: 0.3024, Train Acc: 0.9295, Val Acc: 0.8914, Val Macro-F1: 0.8903\n",
      "Epoch 5/10 - Train Loss: 0.2013, Val Loss: 0.3023, Train Acc: 0.9295, Val Acc: 0.8934, Val Macro-F1: 0.8918\n",
      "Epoch 6/10 - Train Loss: 0.1992, Val Loss: 0.3020, Train Acc: 0.9306, Val Acc: 0.8928, Val Macro-F1: 0.8919\n",
      "Epoch 7/10 - Train Loss: 0.1939, Val Loss: 0.3031, Train Acc: 0.9316, Val Acc: 0.8922, Val Macro-F1: 0.8910\n",
      "Epoch 8/10 - Train Loss: 0.1930, Val Loss: 0.2987, Train Acc: 0.9322, Val Acc: 0.8948, Val Macro-F1: 0.8937\n",
      "Epoch 9/10 - Train Loss: 0.1924, Val Loss: 0.3003, Train Acc: 0.9332, Val Acc: 0.8947, Val Macro-F1: 0.8937\n",
      "Epoch 10/10 - Train Loss: 0.1892, Val Loss: 0.2989, Train Acc: 0.9339, Val Acc: 0.8948, Val Macro-F1: 0.8937\n",
      "New best model found: LR: 0.001, Arch: 1 with Val Macro-F1: 0.8937\n",
      "\n",
      "Training model with architecture 1 and learning rate 0.0005\n",
      "Epoch 1/10 - Train Loss: 0.1950, Val Loss: 0.3005, Train Acc: 0.9335, Val Acc: 0.8944, Val Macro-F1: 0.8927\n",
      "Epoch 2/10 - Train Loss: 0.1936, Val Loss: 0.3058, Train Acc: 0.9292, Val Acc: 0.8926, Val Macro-F1: 0.8925\n",
      "Epoch 3/10 - Train Loss: 0.1922, Val Loss: 0.3026, Train Acc: 0.9335, Val Acc: 0.8945, Val Macro-F1: 0.8930\n",
      "Epoch 4/10 - Train Loss: 0.1861, Val Loss: 0.3020, Train Acc: 0.9351, Val Acc: 0.8932, Val Macro-F1: 0.8922\n",
      "Epoch 5/10 - Train Loss: 0.1852, Val Loss: 0.3009, Train Acc: 0.9356, Val Acc: 0.8936, Val Macro-F1: 0.8920\n",
      "Epoch 6/10 - Train Loss: 0.1844, Val Loss: 0.2991, Train Acc: 0.9369, Val Acc: 0.8946, Val Macro-F1: 0.8935\n",
      "Epoch 7/10 - Train Loss: 0.1816, Val Loss: 0.2995, Train Acc: 0.9375, Val Acc: 0.8950, Val Macro-F1: 0.8937\n",
      "Epoch 8/10 - Train Loss: 0.1807, Val Loss: 0.3005, Train Acc: 0.9367, Val Acc: 0.8958, Val Macro-F1: 0.8943\n",
      "Epoch 9/10 - Train Loss: 0.1804, Val Loss: 0.2999, Train Acc: 0.9370, Val Acc: 0.8956, Val Macro-F1: 0.8947\n",
      "Epoch 10/10 - Train Loss: 0.1790, Val Loss: 0.2982, Train Acc: 0.9375, Val Acc: 0.8952, Val Macro-F1: 0.8940\n",
      "New best model found: LR: 0.0005, Arch: 1 with Val Macro-F1: 0.8947\n",
      "\n",
      "Training model with architecture 2 and learning rate 0.005\n",
      "Epoch 1/10 - Train Loss: 0.8252, Val Loss: 0.4804, Train Acc: 0.8285, Val Acc: 0.8293, Val Macro-F1: 0.8298\n",
      "Epoch 2/10 - Train Loss: 0.4531, Val Loss: 0.4172, Train Acc: 0.8579, Val Acc: 0.8488, Val Macro-F1: 0.8443\n",
      "Epoch 3/10 - Train Loss: 0.3970, Val Loss: 0.3781, Train Acc: 0.8719, Val Acc: 0.8633, Val Macro-F1: 0.8593\n",
      "Epoch 4/10 - Train Loss: 0.3272, Val Loss: 0.3668, Train Acc: 0.8787, Val Acc: 0.8662, Val Macro-F1: 0.8654\n",
      "Epoch 5/10 - Train Loss: 0.3100, Val Loss: 0.3605, Train Acc: 0.8847, Val Acc: 0.8679, Val Macro-F1: 0.8631\n",
      "Epoch 6/10 - Train Loss: 0.2994, Val Loss: 0.3406, Train Acc: 0.8935, Val Acc: 0.8770, Val Macro-F1: 0.8744\n",
      "Epoch 7/10 - Train Loss: 0.2703, Val Loss: 0.3171, Train Acc: 0.9042, Val Acc: 0.8839, Val Macro-F1: 0.8828\n",
      "Epoch 8/10 - Train Loss: 0.2626, Val Loss: 0.3150, Train Acc: 0.9064, Val Acc: 0.8849, Val Macro-F1: 0.8842\n",
      "Epoch 9/10 - Train Loss: 0.2570, Val Loss: 0.3117, Train Acc: 0.9082, Val Acc: 0.8852, Val Macro-F1: 0.8846\n",
      "Epoch 10/10 - Train Loss: 0.2411, Val Loss: 0.3128, Train Acc: 0.9127, Val Acc: 0.8855, Val Macro-F1: 0.8834\n",
      "\n",
      "Training model with architecture 2 and learning rate 0.0025\n",
      "Epoch 1/10 - Train Loss: 0.2731, Val Loss: 0.3935, Train Acc: 0.8770, Val Acc: 0.8537, Val Macro-F1: 0.8517\n",
      "Epoch 2/10 - Train Loss: 0.2629, Val Loss: 0.3229, Train Acc: 0.9066, Val Acc: 0.8832, Val Macro-F1: 0.8823\n",
      "Epoch 3/10 - Train Loss: 0.2564, Val Loss: 0.3258, Train Acc: 0.9062, Val Acc: 0.8811, Val Macro-F1: 0.8816\n",
      "Epoch 4/10 - Train Loss: 0.2256, Val Loss: 0.3055, Train Acc: 0.9195, Val Acc: 0.8878, Val Macro-F1: 0.8862\n",
      "Epoch 5/10 - Train Loss: 0.2179, Val Loss: 0.3191, Train Acc: 0.9151, Val Acc: 0.8837, Val Macro-F1: 0.8820\n",
      "Epoch 6/10 - Train Loss: 0.2135, Val Loss: 0.3023, Train Acc: 0.9243, Val Acc: 0.8909, Val Macro-F1: 0.8899\n",
      "Epoch 7/10 - Train Loss: 0.1974, Val Loss: 0.3031, Train Acc: 0.9287, Val Acc: 0.8920, Val Macro-F1: 0.8908\n",
      "Epoch 8/10 - Train Loss: 0.1943, Val Loss: 0.2955, Train Acc: 0.9314, Val Acc: 0.8948, Val Macro-F1: 0.8936\n",
      "Epoch 9/10 - Train Loss: 0.1909, Val Loss: 0.2990, Train Acc: 0.9313, Val Acc: 0.8932, Val Macro-F1: 0.8924\n",
      "Epoch 10/10 - Train Loss: 0.1827, Val Loss: 0.2946, Train Acc: 0.9355, Val Acc: 0.8956, Val Macro-F1: 0.8945\n",
      "\n",
      "Training model with architecture 2 and learning rate 0.001\n",
      "Epoch 1/10 - Train Loss: 0.1948, Val Loss: 0.3082, Train Acc: 0.9303, Val Acc: 0.8924, Val Macro-F1: 0.8918\n",
      "Epoch 2/10 - Train Loss: 0.1912, Val Loss: 0.3081, Train Acc: 0.9314, Val Acc: 0.8907, Val Macro-F1: 0.8877\n",
      "Epoch 3/10 - Train Loss: 0.1869, Val Loss: 0.3104, Train Acc: 0.9311, Val Acc: 0.8947, Val Macro-F1: 0.8920\n",
      "Epoch 4/10 - Train Loss: 0.1726, Val Loss: 0.3025, Train Acc: 0.9384, Val Acc: 0.8945, Val Macro-F1: 0.8941\n",
      "Epoch 5/10 - Train Loss: 0.1696, Val Loss: 0.3057, Train Acc: 0.9401, Val Acc: 0.8962, Val Macro-F1: 0.8955\n",
      "Epoch 6/10 - Train Loss: 0.1676, Val Loss: 0.3011, Train Acc: 0.9407, Val Acc: 0.8964, Val Macro-F1: 0.8952\n",
      "Epoch 7/10 - Train Loss: 0.1602, Val Loss: 0.3008, Train Acc: 0.9442, Val Acc: 0.8980, Val Macro-F1: 0.8968\n",
      "Epoch 8/10 - Train Loss: 0.1577, Val Loss: 0.3016, Train Acc: 0.9448, Val Acc: 0.8978, Val Macro-F1: 0.8960\n",
      "Epoch 9/10 - Train Loss: 0.1568, Val Loss: 0.3019, Train Acc: 0.9458, Val Acc: 0.8974, Val Macro-F1: 0.8964\n",
      "Epoch 10/10 - Train Loss: 0.1527, Val Loss: 0.3015, Train Acc: 0.9468, Val Acc: 0.8968, Val Macro-F1: 0.8957\n",
      "New best model found: LR: 0.001, Arch: 2 with Val Macro-F1: 0.8968\n",
      "\n",
      "Training model with architecture 2 and learning rate 0.0005\n",
      "Epoch 1/10 - Train Loss: 0.1612, Val Loss: 0.3061, Train Acc: 0.9438, Val Acc: 0.8970, Val Macro-F1: 0.8953\n",
      "Epoch 2/10 - Train Loss: 0.1586, Val Loss: 0.3079, Train Acc: 0.9459, Val Acc: 0.8960, Val Macro-F1: 0.8950\n",
      "Epoch 3/10 - Train Loss: 0.1564, Val Loss: 0.3185, Train Acc: 0.9415, Val Acc: 0.8917, Val Macro-F1: 0.8911\n",
      "Epoch 4/10 - Train Loss: 0.1482, Val Loss: 0.3093, Train Acc: 0.9488, Val Acc: 0.8957, Val Macro-F1: 0.8941\n",
      "Epoch 5/10 - Train Loss: 0.1462, Val Loss: 0.3084, Train Acc: 0.9491, Val Acc: 0.8967, Val Macro-F1: 0.8957\n",
      "Epoch 6/10 - Train Loss: 0.1450, Val Loss: 0.3076, Train Acc: 0.9502, Val Acc: 0.8966, Val Macro-F1: 0.8955\n",
      "Epoch 7/10 - Train Loss: 0.1406, Val Loss: 0.3062, Train Acc: 0.9520, Val Acc: 0.8979, Val Macro-F1: 0.8970\n",
      "Epoch 8/10 - Train Loss: 0.1398, Val Loss: 0.3049, Train Acc: 0.9521, Val Acc: 0.8994, Val Macro-F1: 0.8984\n",
      "Epoch 9/10 - Train Loss: 0.1390, Val Loss: 0.3082, Train Acc: 0.9524, Val Acc: 0.8962, Val Macro-F1: 0.8955\n",
      "Epoch 10/10 - Train Loss: 0.1367, Val Loss: 0.3061, Train Acc: 0.9524, Val Acc: 0.8975, Val Macro-F1: 0.8965\n",
      "New best model found: LR: 0.0005, Arch: 2 with Val Macro-F1: 0.8984\n",
      "\n",
      "Training model with architecture 3 and learning rate 0.005\n",
      "Epoch 1/10 - Train Loss: 0.5571, Val Loss: 0.4136, Train Acc: 0.8597, Val Acc: 0.8526, Val Macro-F1: 0.8488\n",
      "Epoch 2/10 - Train Loss: 0.4165, Val Loss: 0.4132, Train Acc: 0.8598, Val Acc: 0.8462, Val Macro-F1: 0.8406\n",
      "Epoch 3/10 - Train Loss: 0.3814, Val Loss: 0.4387, Train Acc: 0.8561, Val Acc: 0.8392, Val Macro-F1: 0.8374\n",
      "Epoch 4/10 - Train Loss: 0.3193, Val Loss: 0.3325, Train Acc: 0.8964, Val Acc: 0.8780, Val Macro-F1: 0.8757\n",
      "Epoch 5/10 - Train Loss: 0.2991, Val Loss: 0.3256, Train Acc: 0.8997, Val Acc: 0.8809, Val Macro-F1: 0.8802\n",
      "Epoch 6/10 - Train Loss: 0.2894, Val Loss: 0.3328, Train Acc: 0.9040, Val Acc: 0.8804, Val Macro-F1: 0.8781\n",
      "Epoch 7/10 - Train Loss: 0.2585, Val Loss: 0.3198, Train Acc: 0.9131, Val Acc: 0.8857, Val Macro-F1: 0.8843\n",
      "Epoch 8/10 - Train Loss: 0.2500, Val Loss: 0.3125, Train Acc: 0.9169, Val Acc: 0.8869, Val Macro-F1: 0.8864\n",
      "Epoch 9/10 - Train Loss: 0.2430, Val Loss: 0.3139, Train Acc: 0.9204, Val Acc: 0.8883, Val Macro-F1: 0.8873\n",
      "Epoch 10/10 - Train Loss: 0.2236, Val Loss: 0.3043, Train Acc: 0.9249, Val Acc: 0.8890, Val Macro-F1: 0.8880\n",
      "\n",
      "Training model with architecture 3 and learning rate 0.0025\n",
      "Epoch 1/10 - Train Loss: 0.2643, Val Loss: 0.3293, Train Acc: 0.9142, Val Acc: 0.8858, Val Macro-F1: 0.8839\n",
      "Epoch 2/10 - Train Loss: 0.2610, Val Loss: 0.3439, Train Acc: 0.9097, Val Acc: 0.8760, Val Macro-F1: 0.8748\n",
      "Epoch 3/10 - Train Loss: 0.2553, Val Loss: 0.3207, Train Acc: 0.9164, Val Acc: 0.8863, Val Macro-F1: 0.8843\n",
      "Epoch 4/10 - Train Loss: 0.2200, Val Loss: 0.3169, Train Acc: 0.9263, Val Acc: 0.8885, Val Macro-F1: 0.8874\n",
      "Epoch 5/10 - Train Loss: 0.2122, Val Loss: 0.3199, Train Acc: 0.9298, Val Acc: 0.8898, Val Macro-F1: 0.8888\n",
      "Epoch 6/10 - Train Loss: 0.2064, Val Loss: 0.3195, Train Acc: 0.9325, Val Acc: 0.8892, Val Macro-F1: 0.8882\n",
      "Epoch 7/10 - Train Loss: 0.1839, Val Loss: 0.3115, Train Acc: 0.9406, Val Acc: 0.8963, Val Macro-F1: 0.8951\n",
      "Epoch 8/10 - Train Loss: 0.1799, Val Loss: 0.3129, Train Acc: 0.9421, Val Acc: 0.8952, Val Macro-F1: 0.8934\n",
      "Epoch 9/10 - Train Loss: 0.1746, Val Loss: 0.3122, Train Acc: 0.9443, Val Acc: 0.8945, Val Macro-F1: 0.8929\n",
      "Epoch 10/10 - Train Loss: 0.1644, Val Loss: 0.3082, Train Acc: 0.9487, Val Acc: 0.8968, Val Macro-F1: 0.8957\n",
      "\n",
      "Training model with architecture 3 and learning rate 0.001\n",
      "Epoch 1/10 - Train Loss: 0.1816, Val Loss: 0.3208, Train Acc: 0.9437, Val Acc: 0.8936, Val Macro-F1: 0.8925\n",
      "Epoch 2/10 - Train Loss: 0.1812, Val Loss: 0.3227, Train Acc: 0.9416, Val Acc: 0.8902, Val Macro-F1: 0.8892\n",
      "Epoch 3/10 - Train Loss: 0.1753, Val Loss: 0.3274, Train Acc: 0.9445, Val Acc: 0.8933, Val Macro-F1: 0.8922\n",
      "Epoch 4/10 - Train Loss: 0.1566, Val Loss: 0.3182, Train Acc: 0.9540, Val Acc: 0.8968, Val Macro-F1: 0.8959\n",
      "Epoch 5/10 - Train Loss: 0.1502, Val Loss: 0.3229, Train Acc: 0.9531, Val Acc: 0.8961, Val Macro-F1: 0.8951\n",
      "Epoch 6/10 - Train Loss: 0.1482, Val Loss: 0.3270, Train Acc: 0.9544, Val Acc: 0.8936, Val Macro-F1: 0.8926\n",
      "Epoch 7/10 - Train Loss: 0.1365, Val Loss: 0.3211, Train Acc: 0.9585, Val Acc: 0.8983, Val Macro-F1: 0.8971\n",
      "Epoch 8/10 - Train Loss: 0.1346, Val Loss: 0.3301, Train Acc: 0.9586, Val Acc: 0.8948, Val Macro-F1: 0.8942\n",
      "Epoch 9/10 - Train Loss: 0.1325, Val Loss: 0.3253, Train Acc: 0.9611, Val Acc: 0.8967, Val Macro-F1: 0.8956\n",
      "Epoch 10/10 - Train Loss: 0.1263, Val Loss: 0.3284, Train Acc: 0.9620, Val Acc: 0.8967, Val Macro-F1: 0.8956\n",
      "\n",
      "Training model with architecture 3 and learning rate 0.0005\n",
      "Epoch 1/10 - Train Loss: 0.1380, Val Loss: 0.3378, Train Acc: 0.9585, Val Acc: 0.8939, Val Macro-F1: 0.8926\n",
      "Epoch 2/10 - Train Loss: 0.1364, Val Loss: 0.3344, Train Acc: 0.9596, Val Acc: 0.8944, Val Macro-F1: 0.8933\n",
      "Epoch 3/10 - Train Loss: 0.1354, Val Loss: 0.3435, Train Acc: 0.9573, Val Acc: 0.8911, Val Macro-F1: 0.8903\n",
      "Epoch 4/10 - Train Loss: 0.1263, Val Loss: 0.3379, Train Acc: 0.9629, Val Acc: 0.8938, Val Macro-F1: 0.8926\n",
      "Epoch 5/10 - Train Loss: 0.1219, Val Loss: 0.3473, Train Acc: 0.9638, Val Acc: 0.8939, Val Macro-F1: 0.8929\n",
      "Epoch 6/10 - Train Loss: 0.1215, Val Loss: 0.3416, Train Acc: 0.9637, Val Acc: 0.8955, Val Macro-F1: 0.8942\n",
      "Epoch 7/10 - Train Loss: 0.1150, Val Loss: 0.3434, Train Acc: 0.9670, Val Acc: 0.8944, Val Macro-F1: 0.8935\n",
      "Epoch 8/10 - Train Loss: 0.1137, Val Loss: 0.3455, Train Acc: 0.9674, Val Acc: 0.8932, Val Macro-F1: 0.8922\n",
      "Epoch 9/10 - Train Loss: 0.1119, Val Loss: 0.3467, Train Acc: 0.9673, Val Acc: 0.8937, Val Macro-F1: 0.8924\n",
      "Epoch 10/10 - Train Loss: 0.1104, Val Loss: 0.3443, Train Acc: 0.9688, Val Acc: 0.8953, Val Macro-F1: 0.8943\n"
     ]
    }
   ],
   "source": [
    "# Define architectures\n",
    "architectures = [\n",
    "    [DenseLayer(784, 128), ReLU(), DenseLayer(128, 10), Softmax()],\n",
    "    [DenseLayer(784, 256), ReLU(), DenseLayer(256, 128), ReLU(), DenseLayer(128, 10), Softmax()],\n",
    "    [DenseLayer(784, 512), ReLU(), BatchNormalization(512), DenseLayer(512, 256), ReLU(), DenseLayer(256, 10), Softmax()]\n",
    "]\n",
    "\n",
    "# Test different learning rates\n",
    "learning_rates = [0.005, 0.0025, 0.001, 0.0005]\n",
    "\n",
    "# Dictionary to store results and the best configuration\n",
    "results = {}\n",
    "best_model_config = None\n",
    "best_model_f1 = 0\n",
    "best_model_path = 'best_model_final.pickle'\n",
    "\n",
    "for i, arch in enumerate(architectures):\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nTraining model with architecture {i+1} and learning rate {lr}\")\n",
    "        network = NeuralNetwork(arch)\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        \n",
    "        # Train the model without saving weights in each epoch\n",
    "        history, max_val_f1 = train_model(\n",
    "            network, X_train, y_train, X_val, y_val, epochs=10, batch_size=64, learning_rate=lr, optimizer=optimizer\n",
    "        , archnum = i+1)\n",
    "        \n",
    "        # Save training history\n",
    "        results[(lr, f\"arch_{i+1}\")] = history\n",
    "\n",
    "        # If this model has the best F1 score across all configurations, save its weights\n",
    "        if max_val_f1 > best_model_f1:\n",
    "            best_model_f1 = max_val_f1\n",
    "            best_model_config = (lr, f\"arch_{i+1}\")\n",
    "            save_model(network, best_model_path)\n",
    "            print(f\"New best model found: LR: {lr}, Arch: {i+1} with Val Macro-F1: {max_val_f1:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics for all architectures and learning rates\n",
    "plot_metric(results, 'train_loss', 'Training Loss over Epochs')\n",
    "plot_metric(results, 'val_loss', 'Validation Loss over Epochs')\n",
    "plot_metric(results, 'train_accuracy', 'Training Accuracy over Epochs')\n",
    "plot_metric(results, 'val_accuracy', 'Validation Accuracy over Epochs')\n",
    "plot_metric(results, 'val_macro_f1', 'Validation Macro F1 Score over Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Configuration: Learning Rate: 0.0005, Architecture: arch_2\n",
      "Best Validation Macro-F1 Score: 0.8984\n"
     ]
    }
   ],
   "source": [
    "# Display the best model configuration\n",
    "print(f\"Best Model Configuration: Learning Rate: {best_model_config[0]}, Architecture: {best_model_config[1]}\")\n",
    "print(f\"Best Validation Macro-F1 Score: {best_model_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8933\n",
      "Test Macro F1 Score: 0.8935\n",
      "Confusion Matrix:\n",
      " [[825   0  21  19   2   1 124   0   8   0]\n",
      " [  2 972   1  19   4   0   1   0   1   0]\n",
      " [ 10   0 822  13  81   0  73   0   1   0]\n",
      " [ 18  10  11 896  33   0  27   1   4   0]\n",
      " [  0   1  85  25 839   0  50   0   0   0]\n",
      " [  0   0   0   1   0 957   0  22   1  19]\n",
      " [ 98   2  71  25  71   0 725   0   8   0]\n",
      " [  0   0   0   0   0  15   0 965   0  20]\n",
      " [  5   1   2   3   3   4   5   4 973   0]\n",
      " [  0   0   0   0   0   5   1  35   0 959]]\n"
     ]
    }
   ],
   "source": [
    "best_model_path = 'best_model_final.pickle'\n",
    "best_network = load_model(best_model_path)\n",
    "\n",
    "# Test set evaluation\n",
    "test_predictions = best_network.forward(X_test, training=False)\n",
    "test_accuracy = accuracy_score(y_test, np.argmax(test_predictions, axis=1))\n",
    "test_macro_f1 = f1_score(y_test, np.argmax(test_predictions, axis=1), average=\"macro\")\n",
    "test_conf_matrix = confusion_matrix(y_test, np.argmax(test_predictions, axis=1))\n",
    "plot_confusion_matrix(test_conf_matrix, \"Test Confusion Matrix\")\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Macro F1 Score: {test_macro_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", test_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
